%!TEX root = Thesis.tex

\chapter{Logical sentiment analysis}

For the purpose of this thesis a \emph{sentiment analysis} is defined as follows:
\begin{definition}
a sentiment analysis $\mathcal{A}$ is a computation on a review text $T \in \Sigma^\star$ with respect to a subject $s \in S$, where $\Sigma^\star$ denotes the set of all permissible texts in the language. The result is an normalized score as shown in (\ref{eq:Analysis}). The yielded score should reflect the \emph{polarity} of the subject in the text, i.e.\ whether the overall opinion is positive (a score close to $\iota$), negative (a score close to $-\iota$), or neutral (a score close to $0$).
  \begin{align}
	 \mathcal{A}: \Sigma^\star \to S \to [-\iota;\iota]
	 \label{eq:Analysis}
  \end{align}
\end{definition}

 It should be evident that this computation is far from triviel, and constitutes the cornerstone of this thesis. There are several steps needed, if such computation should yield any reasonable result. As mentioned in the introduction the goal is a logical approach for achieving this. The following outlines the overall steps to be completed, their associated problematics in this process, and succintively presents different approaches to solve each step. The chosen approach for each step will be presented in much more details in later chapters. Finally ... \todo[inline]{Something about data acquisition of test-data}

\clearpage

\section{Syntactic analysis}
The first step is to determine the grammatical structure of the input texts with respect to the rules of the English language. It is expected that the reader is familiar with English grammar rules and syntactic categories, including phrasal categories and lexical categories (also called parts of speech). As mentioned erlier it is essential that the chosen solution is able to cope with \emph{real} data, collected from actual review scenarios. This implies a robust syntactic analysis accepting a large vocabulary and a wide range of sentence structures. In order to calculate the actual polarity it is essential to have semantic annotations on the lexical units. It is argued that a feasible and suitable solution is to use a grammar that is \emph{lexicalized}, i.e.\ where the rules are essentially langauge independent, and the syntactic properties are derived from a lexicon. Thus the development of a lexicalized grammar is mainly a task of acquiring a sutable lexicon for the desired language.

Even though the task of syntactic analysis now is largely reduced to a task of lexicon acquisition, which will be addressed in a moment, there are still general concerns that are worth acknowledging. Simply identifying the different sentences and lexical units in a text can yield a difficult task. Consider for intance the text (\ref{ex:SentenceBoundary}) which is taken from the Wall Street Journal corpus \cite{wsjCorpus}. There are six periods in it, but only two of them indicates sentence bounderies, and delimits the passage into its two sentences.
\begin{numquote}
  Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.  
  \label{ex:SentenceBoundary}
\end{numquote}

\citeauthor{ectendingCCG}~\shortcite[p.~108-110]{ectendingCCG}\ identifies several issues in being able to efficiently handle natural language texts solely with lexicalized grammars, mainly due to the need for entries for various combinations of proper nouns, abbreviated terms, dates, numbers, etc. Instead they suggest to use pattern matching and statistical techniques as a preprocessing step, for which efficient components exists, which translate into reduced complexity for the actual syntactic analysis.

Fortunately the domain of small review texts allows some restrictions and assumptions, that at least will ease some of the issues in this context. For instance it seems like a valid assumption that most of the review texts will consists of at most two sentences. Its is argued that this indeed is achievable by sufficient intructing and constraining the reviewers duing data collection, e.g.\ only allowing up to a curtain number of characters. It is also argued that the use of proper nouns and specific dates can be fairly limited, in that a context have already been established for the reviewer cf. section~\ref{sec:naturalDataCollection}. However the domain of small review texts also introduce concerns that are absent from other domains, including the possebility of incorrect grammar and spelling, since the texts comes unedited from humans with varying English skills. A solution that would only work on \emph{perfect texts} (i.e.\ texts of sentences with completely correct grammar and spelling) would not be adequate. In order to at least try to handle minor misspellings it is intended to use algoritms that can select alternatives from the lexicon. Reasons for this could be that word is simply unpresent from the system's vocabulary (e.g.\ misspelled), or on a grammatical incorrect form (e.g.\ wrong person, gender, tense, case, etc.). 

\subsection{Mildly context-sensitive grammars}
There exists formal proofs that some natural language structures requires formal power beyond \emph{context-free grammars} (CFG), i.e.\ \cite{nlpNotCFG} and \cite{nlpNotCFG2}. Thus the search for grammars with more expressive power has long been a major study within the field of computational linguistics. The goal is a grammar that is so restrive as possible, allowing efficient syntactic analysis, but still capable of capturing these structures. The class of \emph{mildly context-sensitive grammars} are conjectured to be powerful enough to model natural languages while remaining efficient with respect to syntactic analysis cf. \cite{mildlyCSG}.

Different grammar formalisms from this class has been considered, including \emph{Tree Adjunct Grammar} (TAG) \cite{tag}, in its lexicalized form (LTAG), \emph{Head Grammar} (HG) \cite{hg} and \emph{Categorial Grammar} (CG) \cite{steedmanDraft}\todo{Find original 1985/1988 article}.
It has been shown that these are all equal in expressive power by \citeauthor{theEquivalence}~\shortcite{theEquivalence}. The grammar formilism chosen for the purpose of this thesis is \emph{Combinatory Categorial Grammar} (CCG), pioneered largely by \citeauthor{sp} \shortcite{sp}. CCG adds a layer of combinatory logic onto pure CG.

\todo[inline]{Explain the choice of CCG}

Chapter~\ref{chap:CCG} will formally introduce the CCG in much more detail.


\subsection{Lexicon acquisition}
As mentioned earlier, acquiring a sutable lexicon is crucial in the development of a lexicalized grammar, especially when the grammar is to accept a large vocabulary and a wide range of sentence structures.
\clearpage

There exists several wide coverage CCG lexicons, most notable \emph{CCGbank}, compiled by \citeauthor{ccgBank} \shortcite{ccgBank} by translating almost the entire Penn Treebank \cite{pennTreebank}, which contains over 4.5 million words. The result is a higly covering lexicon, with some entries having assigned over 100 different lexical categories. Unfortunately these lexicons are not free to use, and it has not been possible to fund a license.

\clearpage

\section{Semantic analysis}



\subsection{Using real data-sets}

\clearpage

\todo[inline]{Continue...} 
\clearpage

\section{Data acquisition}
In order to sucesfully perform the proposed computation, and thus the sentiment analysis 

\section{Tagged corpora / Part-of-speech tagging}

Since English is 

\subsection{The Brown Corpus}
The Brown Corpus was compiled by \citeauthor{brown} \shortcite{brown} by collecting written works printed in United States during the year 1961. The corpus consists of just over one million words taken from 500 American English sample texts, with the intension of covering a highly representative variety of writing styles and sentence structures.

Notable drawbacks of the Brown Corpus include its age, i.e.\ there are evidently review topics where essential and recurring words used in present day writing was not coined yet or rarely used back 50 years ago. For instance does the Brown Corpus not recognize the words \emph{internet}, \emph{hotspot}

   sentences will containing words has found it's way into comon  that 

Other corpora has been considered

\begin{quote}
  \it As with English around the world, the English language as used in the United Kingdom and the Republic of Ireland is governed by convention rather than formal code: there is no equivalent body to the Académie française or the Real Academia Española, and the authoritative dictionaries (for example, Oxford English Dictionary, Longman Dictionary of Contemporary English, Chambers Dictionary, Collins Dictionary) record usage rather than prescribe it. In addition, vocabulary and usage change with time; words are freely borrowed from other languages and other strains of English, and neologisms are frequent.
  \flushright{\footnotesize\url{http://en.wikipedia.org/wiki/British_English#Standardisation}}
\end{quote}
 
\section{Test dataset}

\subsection{The Opinosis Dataset}
The suggested data set to use is the \emph{Opinosis Dataset}, originally used by \cite{Opinosis}. The data set consists of texts from actual user reviews on a total of 51 different topics. The topics are ranging over different objects, from consumer electronics (e.g.\ GPS navigation, music players, etc.) to hotels, restaurants and cars. For most of the objects, reviews are covered by multiple topics. For instance a specific car is covered by the topics \emph{comfort}, \emph{interior}, \emph{mileage}, \emph{performance}, and \emph{seats}.

It has been hard to find any real alternatives for the \emph{Opinosis Dataset} for several reasons: Most collected reviews are commercial, and thus not free to use; furthermore the \emph{Opinosis Dataset} also contains summerized texts for each of its topics, which are constructed by manual, human interpretation. The latter allow a straight approach for comparison of any results the proposed system will yield.\\
\begin{align}
  &\textit{The hotel buffet had fabulous food.}
  \label{txt:Ex1} \\[3mm]  
  &\textit{Very friendly servers and nice selection of food at a reasonable price.}
  \label{txt:Ex2} \\[3mm]  
  &\textit{Room service was extortionate though, very very expensive,} \nonumber \\
  &\textit{so we didnt bother, as food outlets a few minutes walk away.}
  \label{txt:Ex3}
\end{align}

The texts (\ref{txt:Ex1}) to (\ref{txt:Ex3}) show actual extracts from the data set for a topic on food quality on the Swissôtel Restaurant. While (\ref{txt:Ex1}) is a valid declarative sentence, (\ref{txt:Ex2}) is not, since it lacks a subject (i.e.\ the restaurant). A coarse review of the text in the dataset reveals that missing subjects are a repeating issue. This might not seem that odd, since many people would implicitly imply the subject from the topic that they are reviewing. Thus text missing subjects can in many cases still be considered as valid sentences with minimal effort. The text (\ref{txt:Ex3}) is on the other hand missing a transitive verb (presumably \emph{are}) from the subordinate clause. In cases where such savere gramatically errors occurs it is sugested to ignore the clause, and try only to analyse the main clause. Furthermore the text (\ref{txt:Ex3}) use repeated adverbs (e.g.\ \emph{very very}) to express intensification, however it should not be any major concern that a verb or adjective are modified multiple times by the \emph{same} adverb, but the intended intensification will probably not be included in the semantic analysis. Thus formalizing such a grammer is mostly a tak od designing such lexicon.

As evendent from these examples far from all texts in the dataset are valid sentences.

