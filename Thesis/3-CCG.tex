%!TEX root = Thesis.tex

\chapter{Combinatory categorial grammar}
\label{chap:CCG}

In this chapter the formalishm of Combinatory Categorial Grammar (CCG) is introduced, and \todo{well, it isn't really applied to the sentiment analysis} based on this applied to the proposed sentiment analysis introduced in the previous chapter. For the purpose of explaining and demonstrating CCG a small fragment of English is used. This allow the usage of a ``handwritten'' lexicon initially. In section \ref{sec:lexiconAcquisition} the issues related to acquiring, and analysing with, a wide coverage lexicon are addressed. For the syntactic analysis a CCG lexicon is defined as follows:

\begin{definition}
A CCG lexicon, $\mathcal{L}_\mathrm{CCG}$, is mapping from a lexical unit, $w \in \Sigma^\star$, to a set of 2-tuples, each containing a lexical category and semantical expression that the unit can entail cf.\ (\ref{eq:Lccg}), where $\Gamma$ denotes the set of lexical and phrasal categories, and $\Lambda$ denotes the set of semantical expressions.
\begin{align}
 \mathcal{L}_\mathrm{CCG}: \Sigma^\star \to \mathcal{P}(\Gamma \times \Lambda)
 \label{eq:Lccg}
\end{align}
\end{definition}

A \emph{tagging} of a lexical unit $w \in \Sigma^\star$ is simply the selection of one of the pairs yielded by $\mathcal{L}_\mathrm{CCG}(w)$. Thus given some ordered set of lexical units, which constitutes the text $T \in \Sigma^\star$ to analyse, there might exists many different taggings. This is simply due to the fact that a lexical unit can entail different lexical categories (e.g.\ ``service'' is both a noun and a verb), and different semantical expressions (e.g.\ the noun ``service'' can both refer to assistance and tableware). The number of taggings can thus be large, but is always finite.

The set of lexical and phrasal categories, $\Gamma$, are of a somewhat advanced structure in the CCG presented, since it follows recent work by \citeauthor{multiModalCCG} \shortcite{multiModalCCG} to incorporate \emph{modalities}. A category is either \emph{primitive} or \emph{compound}. The set of primitive categories, $\Gamma_\mathrm{prim} \subset \Gamma$, is language dependent and, for the English desired to be covered by this thesis, it consists of $\cat{S}$ (sentence), $\cat{NP}$ (noun phrase), $\cat{N}$ (noun) and $\cat{PP}$ (prepositional phrase). Compound categories are recursively defined by the infix operators $/_\iota$ (forward slash) and $\bsl_\iota$ (backward slash), i.e.\ if $\alpha$ and $\beta$ are members of $\Gamma$, then so are $\alpha/_\iota\beta$ and $\alpha \bsl_\iota \beta$. This allows the formation of all other lexical and phrasal categories needed. The operators are left associative, but to avoid confusion inner compound categories are always encapsulated in parentheses througout this thesis.

The basic intuitive interpretation of $\alpha /_\iota \beta$ and $\alpha \bsl_\iota \beta$ is as a function that takes a category $\beta$ as argument and yields a result of category $\alpha$. Thus the argument is always stated on the right side of the operators, and the result on the left. The operator determines the dictionality of the application, i.e.\ \emph{where} the argument should appear relative to the function: the forward operator ($/_\iota$) denotes that the argument must appear on the right of the function, whereas the backward operator ($\bsl_\iota$) denotes that the argument must appear on the left. The subscript, $\iota$, denotes the \emph{modality} of the operator, which is a member of a finite set of modalities $\mathcal{M}$ and will be utilized to restrict acceptence in the next section. 

The syntactic categories constitutes a type system for the semantic expressions, with a set of primitive types $\{ \tau_x \; | \; x \in \Gamma_\mathrm{prim} \}$. Thus, if a lexicon entry has category $(\cat{S} \bsl_\iota \cat{NP}) /_\iota (\cat{S} \bsl_\iota \cat{NP})$ then the associated semantic expression must honor this, and have type $(\tau_\cat{NP} \to \tau_\cat{S}) \to \tau_\cat{NP} \to \tau_\cat{S}$ ($\to$ is right assosiative). This is a result of the \emph{Principle of Categorial Type Transparency} \cite[Montague, 1974]{??}. For now it is sufficient to describe the set of semantic expressions, $\Lambda$, as the set of \emph{simply-typed} $\lambda$-expressions, $\Lambda'$, cf.\ definition \ref{def:Lambda}. In chapter \ref{chap:SemanticNetworks} this is extended to support the desired sentiment analysis.

\begin{definition}
The set of $\lambda$-expressions, $\Lambda'$, is defined recursively as the set of expressions $e$, where $e$ is either a variable $x$ from an infinite set of variables $V = \{ v_1, v_2, \ldots \}$, a function abstraction, or a functional application. TODO
\begin{align}
 x \in V                          &\quad \Rightarrow \quad  x \in \Lambda' \tag{Variable} \\
 x \in V, \; M \in \Lambda'          &\quad \Rightarrow \quad  \lambda x . M \in \Lambda' \tag{Abstraction} \\
 M \in \Lambda', \; N \in \Lambda'   &\quad \Rightarrow \quad  (M N) \in \Lambda' \tag{Application} 
 \label{eq:Lambda}
\end{align}
\label{def:Lambda}
\end{definition}

\section{Combinatory rules}
CCGs can be seen as a deductive proof system where the axioms are members of $\Gamma \times \Lambda$. A text $T \in \Sigma^\star$ is accepted as a sentence in the language, if there exists a deductive proof for $\cat{S}$, for some tagging of $T$.

The inference rules of the proof system are known as \emph{combinators}, since they take one or more function pairs, in the form of instances of $\Gamma \times \Lambda$, and produces new instances from the same set. The combinators determines the expressive power of the grammar. A deep presentation of which rules are \emph{needed}, and thus the linguistic motivation behind this, is out of the scope of this thesis. In the following essential combinators covered by \citeauthor{ts} \shortcite[chap.~6]{ts} are succinctly described, which constitutes a \emph{midely context-sensitive} class grammar. These are the development of the combinatory rules \citeauthor{sp} presented in \shortcite[chap.~3]{sp}, however with significant changes with respect to coordinating conjucntions, due to the introduction of modalities on the infix operators. 

The set of modalities used, $\mathcal{M}$, follows \cite{multiModalCCG} and \cite{ts}, where $\mathcal{M} = \{ \star, \diamond, \times, \cdot \}$. The set is partially ordered cf.\ lattice (\ref{eq:modalities}).
\begin{equation}  
  \begin{tikzpicture}
    \tikzstyle{all nodes}=[inner sep=4pt]
    \draw node(*)at(0,1){$\star$}
          node(D)at(-1,0){$\diamond$}                    
          node(X)at(1,0){$\times$}
          node(d)at(0,-1){$\cdot$};
    \draw(*)--(D);
    \draw(*)--(X);
    \draw(D)--(d);
    \draw(X)--(d);
  \end{tikzpicture}
  \label{eq:modalities}
\end{equation}

The basic concept of annotating the infix operators with $\iota \in \mathcal{M}$, is to restrict the application of inferrence rules during deduction in order ensure the soundness of the system. The $\star$ is the most restrictive, allowing only basic rules, $\diamond$ allows rules which perserves the word order, $\times$ allows rules which permutate the word order, and finally $\cdot$ allows any rule without restrictions. The partial ordering allows the most restrictive categories to also be included in the less restrictive, e.g.\ any rule that assumes $\alpha /_\diamond \beta$ will also be valid for $\alpha /_\star \beta$. Since $\cdot$ permits any rule it is convenient to simply write $/$ and $\bsl$ instead of respectively $/_\cdot$ and $\bsl_\cdot$, i.e.\ the dot is omitted from these operators.

The simplest combinator is the \emph{functional application}, which simply allows the instances to be used as functions and arguments, as already described. The forward and backward functional application combinator can be formulated as respectivly ($>$) and ($<$), where $X$ and $Y$ are variables ranging over lexical and phrasal categories, and $f$ and $a$ are variables ranging over semantical expressions. Since the operators are annotated with $\star$, the rules can apply to even the most restrictive categories. For readability instances $(\alpha, e)$ of $\Gamma \times \Lambda$ is written $\alpha : e$. Notice that since the semantical expressions are typed, the application of $a$ to $f$ is sound. 
\begin{align*}
  \catvar{X}/_\star \catvar{Y} : \fvar{f}  \quad\quad                  \catvar{Y} : \fvar{a} 
  &\quad\Rightarrow\quad
  \catvar{X} : \fvar{f} \fvar{a} 
  \tag{$>$} \\
  \catvar{Y}            : \fvar{a}  \quad\quad  \catvar{X} \bsl_\star \catvar{Y} : \fvar{f}
  &\quad\Rightarrow\quad
  \catvar{X} : \fvar{f} \fvar{a}
  \tag{$<$}
\end{align*}

With only these two simple combinatory rules, ($>$) and ($<$), the system is capable of capturing any context-free langauge cf.\ \citeauthor{sp} \shortcite[p.~34]{sp}. For the fragment of English, used to demonstrate CCG, the lexicon is considered to be finite, and it is thus possible, and also convinient, to simply write the mapping of entailment as a subset of $\Sigma^\star \times \Gamma \times \Lambda$. Figure \ref{fig:TinyLex} shows a fragment of this demonstration lexicon. For readability, instances $(w, \alpha, e)$ of $\Sigma^\star \times \Gamma \times \Lambda$ is written $w \models \alpha : e$. Notice that the semantical expressions does not include a body, since it for now is sufficient that just the type of the expressions is correct.
\begin{figure}[ht]
\vspace{-.5em}
\begin{align*}
  \token{the}       &\models \cat{NP} /_\diamond \cat{N}    : \lambda x : \tau_\cat{N} . (\ldots)    \tag{Determiners} \\
  \token{an}        &\models \cat{NP} /_\diamond \cat{N}    : \lambda x : \tau_\cat{N} . (\ldots)    \\
  \token{hotel}     &\models \cat{N}               : (\ldots)                 \tag{Nouns} \\
  \token{service}   &\models \cat{N}               : (\ldots)                 \\
  \token{had}       &\models (\cat{S} \bsl \cat{NP})/\cat{NP}
                                                  : \lambda x : \tau_\cat{NP} . \lambda y : \tau_\cat{NP} . (\ldots)               \tag{Transative verbs} \\
  \token{exceptional}   &\models \cat{N/N}         : \lambda x : \tau_\cat{N} (\ldots)                 \tag{Adjectives}  
\end{align*}
\vspace{-1em}
\caption{A fragment of a tiny handwritten lexicon.}
\label{fig:TinyLex}
\end{figure}

The lexicon for instance shows how determiners can be modeled by the category which takes a noun on the right and yields a noun phrase. Likewise a transitive verb is modeled by a category which first takes a noun phrase on the right (the object), then a noun phrase on the left (the subject) and lastly yields a sentence. Figure \ref{fig:simpleSentence} shows the deduction of $S$ from the simple declarative sentence ``the hotel had an exceptional service'' (semantics are omitted).
\vfill
\begin{figure}[ht]
\center
\scalebox{.7}{
$
  \inference[<]{\inference[>]{\inference{\token{the}}{\cat{NP}/_\diamond\cat{N}}\inference{\token{hotel}}{\cat{N}}}{\cat{NP}}\inference[>]{\inference{\token{had}}{(\cat{S} \bsl \cat{NP})/\cat{NP}}\inference[>]{\inference{\token{an}}{\cat{NP}/_\diamond\cat{N}}\inference[>]{\inference{\token{exceptional}}{\cat{N}/\cat{N}}\inference{\token{service}}{\cat{N}}}{\cat{N}}}{\cat{NP}}}{\cat{S} \bsl \cat{NP}}}{\cat{S}}
$
}
\caption{Deduction of simple declerative sentence.}
\label{fig:simpleSentence}
\end{figure}
\vfill
\clearpage

Besides functional application, CCG also has a set of more restrictive rules, including \emph{functional composition}, defined by the forward and backward functional composition combinators, respectively (${>_{\bf B}}$) and (${<_{\bf B}}$), where $Z$ likewise is variable ranging over $\Gamma$, and $g$ over $\Lambda$.
\begin{align*}
  \catvar{X}   /_\diamond  \catvar{Y} : \fvar{f} \quad \catvar{Y}  /_\diamond   \catvar{Z} : \fvar{g}
  &\quad\Rightarrow\quad
  \catvar{X}   /_\diamond  \catvar{Z} : \lambda a . f ( g \; a)
  \tag{${>_{\bf B}}$} \\
  \catvar{Y} \bsl_\diamond \catvar{Z} : \fvar{g} \quad \catvar{X} \bsl_\diamond \catvar{Y} : \fvar{f} 
  &\quad\Rightarrow\quad
  \catvar{X} \bsl_\diamond \catvar{Z} : \lambda a . f ( g \; a)
  \tag{${<_{\bf B}}$}
\end{align*}
\vspace{-1.5em}

Notice that the semantical expression yielded by (${>_{\bf B}}$) and (${<_{\bf B}}$) is equivalent to regular functional composition ($\circ$) of $f$ and $g$, but since $f \circ g \not \in \Lambda$ they need to be written as $\lambda$-expressions.

Functional composition is often used in connection with another rule, namely \emph{type-raising}, defined by the forward and backward type-raising combinators, respectively (${>_{\bf T}}$) and (${<_{\bf T}}$), where $T$ is a variable ranging over categories.
\begin{align*}
  \catvar{X} : \fvar{a}
  &\quad\Rightarrow\quad
  \catvar{T}/_\iota ( \catvar{T} \bsl_\iota \catvar{X} ) : \lambda f . f a
  \tag{$>_\mathbf{T}$} \\
  \catvar{X} : \fvar{a}
  &\quad\Rightarrow\quad
  \catvar{T}\bsl_\iota ( \catvar{T} /_\iota \catvar{X} ) : \lambda f . f a
  \tag{$<_\mathbf{T}$}
\end{align*}
\vspace{-1.5em}

Type-rasing allows a, often primitive category, to ... $\iota$ is ... (unpredctable?) and is thus often suppressed. ... \todo{Finish} 

\todo[inline]{Re-write: The introduction of function composition into a categorial grammar leads to many kinds of derivational ambiguity that are vacuous in the sense that they do not correspond to semantic ambiguities.
} 
\begin{figure}[ht]
\begin{minipage}[b]{0.5\linewidth}
\center
\scalebox{.7}{
$
  \inference[<]{  
    \inference{\token{the} \; \token{hotel}}{\cat{NP}}
    \inference[>]{
      \inference{\token{provided}}{(\cat{S} \bsl \cat{NP})/\cat{NP}}
      \inference{\token{a} \; \token{service}}{\cat{NP}}
    }{
      \cat{S} \bsl_\diamond \cat{NP}
    }
  }{
    \cat{S}
  }
$
}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
\center
\scalebox{.7}{
$
  \inference[<]{
    \inference[>_B]{  
      \inference[>_T]{  
       \inference{\token{the} \; \token{hotel}}{\cat{NP}}
      }{
        \cat{S} / (\cat{S} \bsl \cat{NP})
      }
      \inference{\token{provided}}{(\cat{S} \bsl \cat{NP})/\cat{NP}}
    }{
      \cat{S} /_\diamond \cat{NP}
    }
    \inference{\token{a} \; \token{service}}{\cat{NP}}
  }{
    \cat{S}
  }
$
}
\end{minipage}
  \caption{Multiple deductions.}
  \label{fig:multipleDeductions}
\end{figure}

A system with these rules demonstrates what is arguably CCG's most unique advantage, namely the ability to handle unbounded dependencies without any additional lexicon entries. For instance a transitive verb, with the \emph{same} category as shown in figure~\ref{fig:TinyLex}, can participate in relative clauses as shown in Example~\ref{ex:relativeClause}.

\begin{figure}[ht]
\vspace{-1em}
\begin{align*}
  \token{that}       &\models (\cat{N} \bsl_\diamond \cat{N})/(\cat{S}/_\diamond\cat{NP})    : \lambda f : (\tau_\cat{NP} \to \tau_\cat{S}) . \lambda x : \tau_\cat{NP} . (\ldots)    \tag{Relative pronouns}   
\end{align*}
\vspace{-1em}
\caption{A fragment of a tiny handwritten lexicon TODO.}
\label{fig:TinyLex2}
\end{figure}

\clearpage
\begin{example}
Figure~\ref{fig:relativeClause} shows an example of both type-rasing and functional composition. The transitive verb (provided) is requiring an object in the form of a noun phrase to its right. However, since it participate in a relative clause, its object is given by the noun that the clause modifies. Type raising allows the subject of the relative clause to raise into a category that can compose with the verb, and thus allows the relative pronoun (that) to bind the relative clause to the noun.
\vfill
\begin{figure}[ht]
\center
\scalebox{.7}{
$
%\inference[>]{
%\inference{\token{the}}{\cat{NP} /_\diamond \cat{N}}
\inference[<]{\inference{\token{service}}{\cat{N}}\inference[>]{\inference{\token{that}}{(\cat{N} \bsl_\diamond \cat{N})/(\cat{S}/_\diamond\cat{NP})}\inference[>B]{\inference[>T]{\inference{\token{the} \; \token{hotel}}{\cat{NP}}}{\cat{S}/(\cat{S} \bsl \cat{NP})}\inference{\token{provided}}{(\cat{S} \bsl \cat{NP})/\cat{NP}}}{\cat{S}/\cat{NP}}}{\cat{N} \bsl_\diamond \cat{N}}}{\cat{N}}
%}{
%  \cat{NP}
%}
$
}
\caption{Deduction of noun phrase with relative clause.}
\label{fig:relativeClause}
\end{figure}
\vfill
\label{ex:relativeClause}
\end{example}

The last set of rules presented here is the \emph{crossed functional composition}, defined by the forward and backward crossed functional composition combinators, respectively (${>_{{\bf B}_\times}}$) and (${<_{{\bf B}_\times}}$).
\begin{align*}
  \catvar{X} /_\times \catvar{Y} : \fvar{f} \quad \catvar{Y} \bsl_\times \catvar{Z} : \fvar{g} 
  &\quad\Rightarrow\quad
  \catvar{X} \bsl_\times \catvar{Z} : \lambda a . f ( g \; a)
  \tag{${>_{\bf{B}_\times}}$} \\
  \catvar{Y}   /_\times  \catvar{Z} : \fvar{g} \quad \catvar{X}  \bsl_\times   \catvar{Y} : \fvar{f}
  &\quad\Rightarrow\quad
  \catvar{X}   /_\times  \catvar{Z} : \lambda a . f ( g \; a)
  \tag{${<_{\bf{B}_\times}}$}
\end{align*}
\todo{Check semantics!}

Crossed functional composition allows \emph{permutation} of the word order. This is usefull to allow adverbs in sentences with shifting of heavy noun phrases as shown in Example~\ref{ex:heavyNP}.

\begin{example}
Normally an adverb is put after the object of the verb it modifies in English, e.g.\ ``the hotel served breakfast daily''. However if the object of the verb becomes ``heavy'' it may sometimes be moved to the end of the sentence, e.g.\ ``the hotel served daily a large breakfast with fresh juice''.

In such cases the adverb needs to compose with the verb, before the verb combines with its object. The crossed functional composition allows exatly such structures as shown in figure~\ref{fig:heavyNP}.

\begin{figure}[ht]
\center
\scalebox{.7}{
$
\inference[<]{\inference{\token{the} \; \token{hotel}}{\cat{NP}} \inference[>]{\inference[<_{B_\times}]{\inference{\token{served}}{(\cat{S} \bsl \cat{NP})/\cat{NP}}\inference{\token{daily}}{(\cat{S} \bsl \cat{NP}) \bsl (\cat{S} \bsl \cat{NP})}}{(\cat{S} \bsl \cat{NP})/\cat{NP}}\inference{\token{a} \; \token{large} \; \token{breakfast} \; \token{with} \; \token{fresh} \; \token{juice}}{\cat{NP}}}{\cat{S} \bsl \cat{NP}}}{\cat{S}}
$
}
\caption{Deduction of ``heavy'' noun phrase shifting.}
\label{fig:heavyNP}
\end{figure}
\label{ex:heavyNP}
\end{example}

\citeauthor{ts} \shortcite{sp,ts} introduces a few additional combinators to capture even more ``exotic'' linguistic phenomena. Recollect that the rules are language independent, and indeed some of the additional phenomena covered by \citeauthor{sp} is either considered infrequent (e.g.\ \emph{parasitic gaps}), or even absent (e.g.\ \emph{cross-serial dependencies}), from the English language desired to cover by this sentiment analysis. It will later be shown (section \ref{sec:TODO}) that the rules already presented indeed cover a substantial part of English.

\section{Coordination}
Coordination by appearance of a coordinating conjunction, such as \emph{and}, \emph{or}, \emph{but}, etc., can be modeled simply by the intuition that such should bind two constituents of same syntactic category, but with different semantical expressions, and yield a result also of that category. Some examples of the \emph{and} coordinating conjunction is shown in figure~\ref{fig:conjunctionLex}.
\begin{figure}[ht]
\vspace{-.5em}
\begin{align*}
  \token{and}       &\models (\cat{S} \bsl_\star \cat{S}) /_\star \cat{S}    : \lambda x : \tau_\cat{S} . \lambda y : \tau_\cat{S} (\ldots)    \tag{Conjunctions} \\
  \token{and}       &\models (\cat{N} \bsl_\star \cat{N}) /_\star \cat{N}    : \lambda x : \tau_\cat{N} . \lambda y : \tau_\cat{N} (\ldots)     \\
  \token{and}       &\models (\cat{NP} \bsl_\star \cat{NP}) /_\star \cat{NP} : \lambda x : \tau_\cat{NP} . \lambda y : \tau_\cat{NP} (\ldots)  \\
  &\hdots 
\end{align*}
\vspace{-1.5em}
\caption{A fragment of a tiny handwritten lexicon TODO.}
\label{fig:conjunctionLex}
\end{figure}

It now becomes evident, why the modalities are needed, since application of the crossed composition combinators without any restrictions could allow scrambled sentences to be deducted falsely, e.g. figure~\ref{fig:withoutModalities}. 
\begin{figure}[ht]
\center
\scalebox{.7}{
$
\inference[<]{
  \inference{
    \token{I}
  }{
    \cat{NP}
  }
  \inference[<]{
    \inference{
      \token{the} \; \token{service}
    }{
      \cat{NP}
    }
    \inference[>_{B_\times}]{
      \inference{
        \token{enjoyed}
      }{
        (\cat{S} \bsl \cat{NP}) / \cat{NP}
      }
      \inference[>]{
        \inference{
          \token{and} 
        }{
          (\cat{NP} \bsl \cat{NP}) / \cat{NP}
        }
        \inference{
          \token{the \; view} 
        }{
          \cat{NP}
        }
      }{
        \cat{NP} \bsl \cat{NP}
      }
    }{
      (\cat{S} \bsl \cat{NP}) \bsl \cat{NP}
    }
  }{
    \cat{S} \bsl \cat{NP}
  }
}{
  \cat{S}
}
$
}
\caption{Without the modalities .}
\label{fig:withoutModalities}
\vspace{-.5em}
\end{figure}

Similar pit-falls are possible if unresticted application of ($>_{\bf B}$) and ($<_{\bf B}$) was allowed, as shown by \citeauthor{baldridgeThesis} \shortcite[chap.~4]{baldridgeThesis} for the Turkish language. This justifies the requirement for the modalities \citeauthor{baldridgeThesis} originally proposed in \shortcite[chap.~5]{baldridgeThesis} and \citeauthor{multiModalCCG} presented in a refined version in \shortcite{multiModalCCG}.
\clearpage

\section{Features and agreement}
In order to ensure that the parsed phrases indeed follows correct English grammar, it is not enough to only consider the phrase structure with respect to the word classes. It is also necessary to 

As stated it is essential to categorize phrases, however it is important to notice, that each category can have numerous arguments i.a. denoting features that apply. For instance Bob Carpenter [1995] states features for person (e.g. 1st, 2nd, or 3rd), number (e.g. singular or plural), and case (e.g. subject or object). The set of features that may apply is language dependent, for instance most indo-european langauges1 has gender features (English being an exception2), but while Danish use common and neuter classes, other languages like German use masculine, feminine, and neuter classes. The important is thus solely that the set of features is always finite – limited by the specific langauge. 




At stated earlier (see section~\ref{sec:PhraseCategories}), each base category is allowed to
carry a finite set of features attached as arguments.




\begin{quote}
  (For example, functional application corresponds to the familiar classical rule of Modus Ponens under this view)
\end{quote}


\todo{Finish}

\cite{cs}

\clearpage








\clearpage

%\begin{scalebox}{.5}


\section{Lexicon acquisition}
\label{sec:lexiconAcquisition}
As mentioned earlier, acquiring a sutable lexicon is crucial in the development of a lexicalized grammar. For small language fragments the lexicon can simply be ``handwritten'', as demonstrated in the previous section. However this is obviously not an sane option when the grammar is to accept a large vocabulary and a wide range of sentence structures.

There exists some wide coverage CCG lexicons, most notable \emph{CCGbank}, compiled by \citeauthor{ccgBank} \shortcite{ccgBank} by translating almost the entire Penn Treebank \cite{pennTreebank}, which contains over 4.5 million words. The result is a higly covering lexicon, with some entries having assigned over 100 different lexical categories. Unfortunately these lexicons are often not free to use, and it has not been possible to fund a license. 

We expect such an algorithm to calculate a match score, that is a weighted average over several metrics. Given below are methods for calculating scores for some evident metrics.

\begin{itemize}
  \item Symbolic similarity -- at its most basic form we can consider a sample string (i.e. a word from an input text) against the system's vocabulary using approximate string matching algorithms such as the  \emph{Levenshtein distance} as described by \cite{Wagner}.

  \item Pronunciation similarity -- it is an valid assumption that many misspellings still share a majority of the pronunciation with the intended word, i.e. they are approximately homophone. Thus comparing the phonetic properties of an sample string with possible matches can in cases correct misspellings. The \emph{Soundex algorithm} by Robert C. Russell and Margaret K. Odell, as described by \cite[p. 391–92]{ACP3}, is a simple, yet power full approach for this purpose.

\end{itemize}

\section{Part of speech tagging}

and  and is a a non-trivial task 

\todo[inline]{C \& C Tools}
\clearpage

\section{Annotating the lexicon}
\begin{center}
\scalebox{.7}{
$
\inference[>]{\inference[>T]{\inference{\inference{\token{Service}\\\pos{NNP}}{\cat{N}_{}:\mathrm{Service}_{0}}}{\cat{NP}_{}:\mathrm{Service}_{0}}}{\cat{S}_{X}/(\cat{S}_{X} \bsl \cat{NP}_{}):\lambda f.(f\;\mathrm{Service}_{0})}\inference[>]{\inference{\token{was}\\\pos{VBD}}{(\cat{S}_{dcl} \bsl \cat{NP}_{})/(\cat{S}_{adj} \bsl \cat{NP}_{}):\lambda x.x}\inference{\token{excellent}\\\pos{JJ}}{\cat{S}_{adj} \bsl \cat{NP}_{}:\lambda x.(x_{\sim 14})}}{\cat{S}_{dcl} \bsl \cat{NP}_{}:\lambda x.(x_{\sim 14})}}{\cat{S}_{dcl}:\mathrm{Service}_{14}}
$
}
\end{center}


