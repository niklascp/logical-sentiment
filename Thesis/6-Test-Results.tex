%!TEX root = Thesis.tex

\chapter{Evaluation}
\label{chap:evaluation}

This chapter evaluates the presented logical approach for sentiment analysis, specifically the \emph{proof of concept} implementation presented in the previous section. In order to truly classify the capabilities of the system, real test-data are needed. In this chapter a test data set is introduced, and results from evaluation of the system on this set are presented. The results are explained, and it is considered weather the robustness and the correctness of the solution is significantly high enough for real applications.

\section{Test data set}
There are several \emph{free to use} labeled review data set available (most of them consisting of movie reviews for some reason). However recall that most research have focused on classifying sentiment on document, sentence or simply on word level, but not on entity level cf.\ Section~\vref{sec:logicalApproach}. This mean that it unfortunately has not been possible to find any free data set there are labeled on entity level. %, and the fact that they are labeled on other levels does not really .

The test data set chosen for evaluation of the system is the \emph{Opinosis Dataset} \cite{Opinosis}. The data set consists of  approximately $\num{7000}$ texts from actual user reviews on number of different topics. The topics are ranging over different product and services, from consumer electronics (e.g.\ GPS navigation, music players, etc.) to hotels and restaurants. They are harvested from several online resellers and service providers, including i.a.\ \emph{Amazon}\footnote{Amazon, \url{http://www.amazon.com/}} and \emph{TripAdvisor}\footnote{TripAdvisor, \url{http://www.tripadvisor.com/}}. The dataset are neither labeled on entity level (or any other level for that matter), since it originally was used for evaluating an \emph{automatic summarization} project by \citeauthor{Opinosis} However the source of the reviews was one of the main reasons to use \emph{Opinosis Dataset} for the evaluation, since it was one of the original goals of the project, that a solution could process real data from actual businesses.
%For most of the product and services, reviews are covered by multiple topics. For instance a specific hotel may be covered by the topics \emph{rooms}, \emph{location}, \emph{price} and \emph{service}. 
%Especially the 
%The data set is collected through a method that closely resembles the \emph{opinion seeking queries} presented back in Section~\vref{sec:naturalDataCollection}. 
%It has been hard to find any real alternatives for the \emph{Opinosis Dataset} for several reasons: Most collected reviews are commercial, and thus not free to use; furthermore the \emph{Opinosis Dataset} also contains summerized texts for each of its topics, which are constructed by manual, human interpretation. The latter allow a straight approach for comparison of any results the proposed system will yield.\\

After a coarse review of the data set it is safe to argue that the data set chosen for evaluation indeed includes all of the problematics discussed in Section~\vref{sec:realData}. Since the data set is unlabeled is was chosen to label a small subset of it in order to measure the robustness and the correctness of the presented solution. To avoid biases toward how the proof of concept system analyzes text the labeling was performed independently by two individuals which had no knowledge of how the presented solution processes texts. As the example texts throughout this thesis might have hinted, the subset chosen was from the set of hotel and restaurant reviews. It was of cause a subset that had not previously been used to test the implementation during development, however fixing the evaluation on the domain of hotel and restaurant reviews allowed the choice of relevant seed concepts cf.\ Section~\ref{sec:sentimentAdj}. The \emph{subject of interest} chosen for the analysis were \emph{rooms}, and the subset was thus randomly sampled from text containing this entity (in any morphological form), since there otherwise are no chance the proof of concept system can yield a result for the text.

The individuals were given a subset of 35 review texts, and should mark each text as either positive, negative or unknown \emph{with respect to the given subject of interest}. Out of the 35 review text the two subject's positive/negative labeling agreed on 34 of them, while unknowns and disagreements were discarded. Thus the inter-human \emph{concordance} for the test data set is $97.1\%$, which is very high, and would arguable drop significantly if more than two individuals were used for label annotation. The full subset samples, as well as each subjects marking is available in Table~\ref{table:labeling} in Appendix~\ref{chap:testData}.

\section{Test results}
The test data set was processed by the proof of concept system, and the raw results are also available Table~\ref{table:labeling}. The system was able to yield an sentiment value for the ``room'' entity for only $32.4\%$ of the test text. Table~\ref{table:results} shows the precision and recall results. As seen the recall is very low, which is addressed in a moment, while it is argued that precision is acceptable. 

\begin{table}[ht]
\center
\begin{tabular}{l|r}
	          & Result \\ \hline
	Precision & $81.8\%$ \\
	Recall    & $26.5\%$ 
\end{tabular}
\caption{Precision and recall results for proof of concept system.}
\label{table:results}
\end{table}